# Copyright (c) Microsoft Corporation.
# Licensed under the MIT License.
import torch
import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self, encoder):
        super().__init__()
        self.encoder = encoder

    def forward(self, code_inputs=None, nl_inputs=None):
        # Ensure exactly one input is provided
        if (code_inputs is None) == (nl_inputs is None):
            raise ValueError("Exactly one of code_inputs or nl_inputs must be provided")

        inputs = code_inputs if code_inputs is not None else nl_inputs

        # Assuming padding token ID is 1; adjust if necessary (common to use 0)
        # Potential issue: If padding token is 0, change to `ne(0)`
        attention_mask = inputs.ne(1)
        
        # Get encoder outputs and extract last hidden states
        encoder_outputs = self.encoder(inputs, attention_mask=attention_mask)
        hidden_states = encoder_outputs.last_hidden_state

        # Mean pooling with attention masking
        # Expand mask to match hidden_states dimensions
        mask_expanded = attention_mask.unsqueeze(-1).expand_as(hidden_states)
        sum_hidden = (hidden_states * mask_expanded).sum(dim=1)
        sum_mask = mask_expanded.sum(dim=1)

        # Avoid division by zero by adding a small epsilon
        sum_mask = torch.clamp(sum_mask, min=1e-9)
        pooled = sum_hidden / sum_mask

        # L2 normalization
        return F.normalize(pooled, p=2, dim=1)